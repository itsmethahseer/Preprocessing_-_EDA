-->Mean removal involves subtracting the mean of the feature from each data point in that feature.
-->Variance scaling involves dividing each data point in a feature by the standard deviation of that feature.
-->Standardization is a combination of mean removal and variance scaling, and involves subtracting the mean and dividing by the standard deviation of each feature. This results in features that are centered around zero and have unit variance. Standardization is a commonly used technique in machine learning and is often applied to features that have significantly different scales or distributions.

-->An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using MinMaxScaler or MaxAbsScaler.

-->One of the standradization technique is feature scaling.The main purpose of feature scaling is to ensure that each feature is measured on a comparable scale so that they can be compared and combined in meaningful ways by machine learning algorithms.

-->if our dataset contains many outliers we want to use Robestscaler.
-->kernel matrix:A kernel matrix is a square matrix that represents the similarity between pairs of data points in a dataset.
--->In Non linear Distribution there are Uniform and Gausian distrbutions.
--> In Uniform distributed data sets , we use a method called QuantileTransformer , which transforms this type data sets into most less in outliers,etc.
-->Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.

-->Normalization:Normalization is the process of scaling individual samples to have unit norm,ie each vector has length 1.For that we use normalize() from the preprocessing of scikit library.

-->Standardization means set the mean=0 and varience=1.
ie,MaxABSScaler->[-1,1]
   Minmaxscaler->[0,1]
If we want to teach a machine text, if we normalize the text by calculating euclidian distance among the words,we can understand it is very less, this is a important of Normalization.
   
-->Feature binarization:Feature binarization is the process of thresholding numerical features to get boolean values.

-->Polynomial Features: It is use to calculate the non linear relationships between the target variable.


-->Exploritory Data Analysis:
It is a process to understand a data in depth and learn different characteristics from the data,often with visual means,This will allow us to build a feel of our data and find a better pattern in it.
        ->it removes irregularities and unregular values from the data.
        ->Help us prepare dataset for our purpose.
        ->Allow on machine learning model to predict a better output.
        ->Give us more accurate result.
        ->Give the relationships between variables.
Steps:
    1)Understand the variables in the dataset:
    2)Cleaning the dataset:may be we want to remove unnecessory columns ,removing the outliers in the dataset etc.
    3)Analysis of relation between variables.
    
    
    
-->Git:
It is a distributed version controle system.
->There are centralized version controle system and distributed version controle system.



->Vectorization concepts:
Vectorization is a fundamental concept in machine learning that involves the transformation of data into vectors or arrays of numerical values that can be easily processed by machine learning algorithms. This process is essential for handling large datasets and performing complex computations efficiently. In this response, I will provide a comprehensive overview of vectorization concepts in machine learning.

1.One-hot Encoding
One-hot encoding is a vectorization technique used to represent categorical data. It involves creating a binary vector where each element represents a unique category in the data. If an example belongs to a particular category, the corresponding element in the vector is set to 1, while all other elements are set to 0. For example, suppose we have a dataset with three categories, "red," "green," and "blue." One-hot encoding would transform these categories into a binary vector of length three, where each element represents one of the categories. For example, the category "red" would be represented as [1, 0, 0], "green" as [0, 1, 0], and "blue" as [0, 0, 1]. One-hot encoding is widely used in natural language processing (NLP) to represent words as numerical vectors.



2.Embeddings
Embeddings are a vectorization technique used to represent high-dimensional data such as text or images in a lower-dimensional space. It involves creating a dense vector for each example, where each element in the vector represents a feature of the example. The goal of embeddings is to capture the semantic meaning of the data, such as the relationships between words in a sentence or the visual features of an image. For example, word embeddings are commonly used in NLP to represent words as dense vectors, where words that have similar meanings are represented by vectors that are close to each other in the vector space.

3.Normalization
Normalization is a vectorization technique used to rescale numerical data so that it has a mean of 0 and a standard deviation of 1. This can be important for machine learning algorithms that are sensitive to the scale of the data. Normalization ensures that the data is centered around 0 and has a standard deviation of 1, which can help to improve the performance of certain machine learning models. For example, neural networks often require normalized inputs to perform well.



4.Feature Scaling
Feature scaling is a vectorization technique used to rescale numerical data to a specific range, such as between 0 and 1. This can be important for machine learning algorithms that use distance-based metrics, as it ensures that all features contribute equally to the final result. Feature scaling can help to avoid bias towards certain features in the dataset and improve the accuracy of the model.


5.Sparse Representation
Sparse representation is a vectorization technique used to represent data that has many zero values, such as sparse matrices. In this technique, only the non-zero values of the data are stored in the vector, which can greatly reduce the memory requirements for large datasets. Sparse representation is widely used in machine learning algorithms that operate on large datasets, such as collaborative filtering and natural language processing.

In summary, vectorization is an essential concept in machine learning that involves transforming data into numerical vectors that can be easily processed by machine learning algorithms. There are several different vectorization techniques that can be used depending on the type of data and the requirements of the machine learning algorithm. These techniques include one-hot encoding, embeddings, normalization, feature scaling, and sparse representation.



-->Performance concepts in machine learning:
Performance concepts are essential in machine learning because they enable us to measure and evaluate the effectiveness of a machine learning model. Here are some key performance concepts in machine learning:

1.Accuracy: Accuracy is the measure of how often a model correctly predicts a target variable. It is expressed as the number of correct predictions divided by the total number of predictions.



2.Precision: Precision is the measure of how many of the positive predictions are actually correct. It is expressed as the number of true positives divided by the sum of true positives and false positives.



3.Recall: Recall is the measure of how many of the actual positives are correctly identified by the model. It is expressed as the number of true positives divided by the sum of true positives and false negatives.

4.F1-score: F1-score is a weighted average of precision and recall, with equal importance given to both measures. It is expressed as the harmonic mean of precision and recall.

5.ROC curve: The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier as its discrimination threshold is varied. It plots the true positive rate against the false positive rate at various threshold settings.


6.AUC: The AUC (Area Under the Curve) is a metric that measures the overall performance of a binary classifier, taking into account all possible threshold settings for the model. It ranges from 0 to 1, with a higher score indicating better performance.

7.Confusion matrix: A confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives of a model.



























